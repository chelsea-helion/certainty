---
title: "Naturalistic Moral Judgment"
author: "Chelsea Helion & William Mitchell"
format: html
editor: visual
---

## Naturalistic moral judgment task

The goal of this project is to examine what features of nmultimodal narrative stimuli (heretofore called "naturalistic") are associated with fluctuations in moral decision-making. To examine this question, we had participants view a series of video clips taken from popular television shows (e.g., "Mindhunter", "The Night Of"). For each video, participants took one of three perspectives, a detective investigating the case, a friend of the victim, or a friend of the accused (randomly assigned, each condition repeated three times). Task code is available here: [SAN Lab Github.](https://github.com/social-and-affective-neuroscience-lab/PythonScripts/tree/main/Certainty%20Task)

### Load relevant packages and load in data

```{r}
#| label: load-packages/data
#| echo: false
#| include: false
library(tidyverse) # for data wrangling and manipulation
library(skimr) # vaguely nicer version of summary stats
library(scales) # for scaling data (entropy)
library(lme4) # multi-level modeling
library(effects) # quick plots
library(mgcv) # for generalized additive mixed models
library(gratia) # for getting the derivatives of the GAMM models
library(MetBrewer) # palettes for plotting
library(monochromeR) # also palettes for plotting
library(brms) # for hierarchical bayesian models
library(tidybayes) # for bayesian analyses
library(janitor) # for making variable names easier to work with
library(ggradar) # for radar plot

df_behav <- read_csv("/Users/tua37526/Dropbox/df_cert_behav.csv")

# select variables related to the certainty task
df_behav <- df_behav %>%
  select(
    PID,
    Age,
    Gender,
    Ethnicity,
    Video,
    Time_video,
    Condition,
    Rating,
    IRI_perstake,
    IRI_fantasy,
    IRI_empath,
    IRI_distress,
    DERS_nonaccept,
    DERS_goals,
    DERS_impulse,
    DERS_aware,
    DERS_strategy,
    DERS_clarity,
    emo_Adoration,
    `emo_Aesthetic Appreciation`,
    emo_Amusement,
    emo_Anxiety,
    emo_Awe,
    emo_Boredom,
    emo_Confusion,
    emo_Craving,
    emo_Disgust,
    `emo_Empathic Pain`,
    emo_Entrancement,
    emo_Excitement,
    emo_Fear,
    emo_Horror,
    emo_Interest,
    emo_Joy,
    emo_Romance,
    emo_Sadness,
    `emo_Sexual Desire`,
    emo_Surprise,
    Entropy,
    Comprehend,
    Engaged,
    Influence_Identity,
    Influence_Information,
    Influence_Context,
    Role_Adopt,
    Role_Maintain,
    Role_Influence,
    Complexity,
    Curiosity,
    Intense_Feel,
    Intense_Video
  )

# change some variable types, sort based on time
df_behav$Video <- as.factor(df_behav$Video)
df_behav <-
  df_behav[order(df_behav$Video, df_behav$PID, df_behav$Time_video),]
df_behav$Condition <- as.factor(df_behav$Condition)
df_behav$Video <- as.factor(df_behav$Video)


```

### Generate summary stats for demographic and task features of the dataset

```{r}
#| echo: false

df_behav %>% 
  filter(Time_video == 10,
         Video == "undoingS1E1") %>%
  summarise(mean_age = mean(Age, na.rm = TRUE),
            sd_age = sd(Age, na.rm = TRUE))

df_behav %>%
  filter(Time_video == 1,
         Video == "broadchurchS1E3") %>%
  {table(.$Gender)}


```

### Summary statistics and comparisons across conditions for task-related judgments

Participants indicated the following on seven-point scales: 1) task engagement, 2) task comprehension, 3) emotional intensity, 4) task-related curiosity, 5) evaluations of task complexity, how influential 6) context, 7) information, and 8) character identity was for their judgments. They also indicated the extent to which the role/condition influenced their judgment, how easy the role/condition was to adopt, and how easy the role/condition was to maintain.

```{r}
#| echo: false

task_ratings <-
  df_behav %>%
  filter(Time_video == 1) %>%
  select(
    PID,
    Condition,
    Comprehend,
    Engaged,
    Influence_Identity,
    Influence_Information,
    Influence_Context,
    Role_Adopt,
    Role_Maintain,
    Role_Influence,
    Complexity,
    Curiosity,
    Intense_Feel,
    Intense_Video
  )

mixed_anova_func <- function(data, within_factors, subject_id) {
  # Initialize an empty list to store the results
  results_list <- list()
  
  # Iterate through the column names in the dataframe
  for (col_name in names(data)) {
    # Check if the column is numeric
    if (is.numeric(data[[col_name]])) {
      # Set up the formula for the mixed ANOVA
      formula <- as.formula(paste(col_name, "~", paste(within_factors, collapse="+"), "+ Error(", subject_id, "/", paste(within_factors, collapse="+"), ")", sep=""))
      
      # Fit the mixed ANOVA model using the "aov()" function
      model <- aov(formula, data = data)
      
      # Add the results to the results list
      results_list[[col_name]] <- summary(model)
    }
  }
  
  # Return the results list
  return(results_list)
}

mixed_anova_func(data = task_ratings, within_factors = "Condition", subject_id = "PID")
```

It looks like the role-based comparisons are significantly different from one another (e.g., role adopt, role maintain, role influence). Plot all of these ratings using a radar plot to get a sense of the distributions.

```{r}

summ_ratings <-
  df_behav %>% 
  group_by(Condition) %>%
  summarise(meanComprehend = mean(Comprehend),
            meanEngaged = mean(Engaged),
            meanInfluenceIdentity = mean(Influence_Identity),
            meanInfluenceInformation = mean(Influence_Information),
            meanInfluenceContext = mean(Influence_Context),
            meanRoleAdopt = mean(Role_Adopt),
            meanRoleMaintain = mean(Role_Maintain),
            meanRoleInfluence = mean(Role_Influence),
            meanComplexity = mean(Complexity),
            meanIntenseFeel = mean(Intense_Feel),
            meanIntenseVideo = mean(Intense_Video))
  

summ_ratings %>%
  ggradar(
    values.radar = c(1, 4, 7),
    axis.label.size = 7,
    grid.label.size = 7,
    axis.labels = c(
      "Comprehend",
      "Engaged",
      "Influenced by \n Identity",
      "Influenced by \n Information",
      "Influenced by \n Context",
      "Ease of \n Adopting Role*",
      "Ease of \n Maintaining Role*",
      "Extent of \n Role Influence*",
      "Complexity",
      "Feeling Intensity",
      "Video Intensity"
    ),
    grid.min = 1,
    grid.mid = 4,
    grid.max = 7,
    group.line.width = 1,
    group.point.size = 3,
    group.colours = c("#00AFBB", "#E7B800", "#FC4E07"),
    background.circle.colour = "white",
    gridline.mid.colour = "grey",
    legend.position = "bottom",
    plot.legend = FALSE
  )


```

### Comparing linear and non-linear functions for time-course analysis

Ok, this is where the fun begins. Given the nature of our data (continuous ratings over a time course) the associations between condition, time, and rating and likely very non-linear. We can compare a variety of polynomial and linear fits to examine whether condition/role is associated with different behavioral time courses (for a great overview of this approach applied to developmental data, see [Nook et al., 2020](https://psycnet.apa.org/record/2019-31446-001)).

Our data is a bit complicated --- we have nine videos, which all differ in terms of their content, and we have three conditions, which could all have different associations. So we will have to fit separate models for all nine videos and all three conditions before we can start to make more general inferences about the nature of the association between role and moral certainty.

```{r}
#| echo: FALSE
#| warning: FALSE

## add polynomial variables
df_behav$time_sq <- df_behav$Time_video ^ 2
df_behav$time_cub <- df_behav$Time_video ^ 3
video_split <-
  split(df_behav, interaction(df_behav$Video, df_behav$Condition))
df_behav$PID <- as.factor(df_behav$PID)

lmer_model_formulas <- list(
  "Rating ~ Time_video + (1|PID)",
  "Rating ~ time_sq + Time_video + (1|PID)",
  "Rating ~ time_cub + time_sq + Time_video + (1|PID)"
)

gam_model_formula <- list("Rating ~ s(Time_video) + s(PID, bs = 're')")

results_list <- list()

# Iterate over each data frame in the list
for (i in seq_along(video_split)) {
  # Create an empty list to store the models for this data frame
  results_list[[i]] <- list()
  
  # Iterate over the lmer model formulas, fit each model, and store the model object in the list
  for (j in seq_along(lmer_model_formulas)) {
    results_list[[i]][["lmer"]][[j]] <-
      lmer(lmer_model_formulas[[j]], data = video_split[[i]])
  }
  
  # Iterate over the gam model formulas, fit each model, and store the model object in the list
  results_list[[i]][["gam"]][[1]] <-
    bam(Rating ~ s(Time_video) + s(PID, bs = 're'), data = video_split[[i]])
}

  
  # Compare models for each split data frame using AIC and BIC
  for (i in seq_along(video_split)) {
    cat("\nComparison for data frame", i, ":\n")
    
    for (j in seq_along(lmer_model_formulas)) {
      lmer_model <- results_list[[i]][["lmer"]][[j]]
      lmer_aic <- AIC(lmer_model)
      lmer_bic <- BIC(lmer_model)
      cat("lmer model", j, "AIC:", lmer_aic, "BIC:", lmer_bic, "\n")
    }
    
    {
      gam_model <- results_list[[i]][["gam"]][[1]]
      gam_aic <- AIC(gam_model)
      gam_bic <- BIC(gam_model)
      cat("gam model", 1, "AIC:", gam_aic, "BIC:", gam_bic, "\n")
    }
  }
  
```

Ok, so for **all** models, the gam fit is superior to the lmer models (linear, quadratic, cubic). This isn't super surprising (though worth noting that some of the linear fits do a pretty decent job for the friend of the victim condition). Now let's take the first derivatives of the models, and see if they plateau at different points based on condition.

```{r}
```

Now cubic models.

```{r}
```
